{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoNCjIZqWqRO"
      },
      "source": [
        "## Required imports and insallations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGbGSwaBdOrt"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "import time\n",
        "\n",
        "\n",
        "!pip install --quiet torch joblib\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIvHDt8dVkUe"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2rXwNsqo7SK"
      },
      "outputs": [],
      "source": [
        "hol = pd.read_pickle('/content/holidays_processed.pkl')\n",
        "fl = pd.read_pickle('/content/flows_processed.pkl')\n",
        "sp = pd.read_pickle('/content/speeds_processed.pkl')\n",
        "wea = pd.read_pickle('/content/weather_processed.pkl')\n",
        "temp = pd.read_pickle('/content/temporal_processed.pkl')\n",
        "f_savg = pd.read_pickle('/content/flows_SAVG_processed.pkl')\n",
        "s_savg = pd.read_pickle('/content/speeds_SAVG_processed.pkl')\n",
        "w_savg = pd.read_pickle('/content/weather_SAVG_processed.pkl')\n",
        "f_mavg = pd.read_pickle('/content/flows_MAVG_processed.pkl')\n",
        "s_mavg = pd.read_pickle('/content/speeds_MAVG_processed.pkl')\n",
        "w_mavg = pd.read_pickle('/content/weather_MAVG_processed.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframes = [hol, fl, sp, wea, temp, f_savg, s_savg, w_savg, f_mavg, s_mavg, w_mavg]\n",
        "for df in dataframes:\n",
        "    df.columns = df.columns.astype(str)"
      ],
      "metadata": {
        "id": "Pah1U6wrpZzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TapXeQA6iO5v"
      },
      "outputs": [],
      "source": [
        "num_nodes = fl.shape[1]\n",
        "print('Number of nodes: ', num_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hd= temp['hour_of_day'].copy().to_frame()\n",
        "su = hol['is_sunday'].copy().to_frame()\n",
        "t = wea['temp'].copy().to_frame()\n",
        "t_savg = w_savg['temp'].copy().to_frame()\n",
        "t_mavg = w_mavg['temp'].copy().to_frame()\n",
        "ws = wea['windspeed'].copy().to_frame()\n",
        "ws_savg = w_savg['windspeed'].copy().to_frame()\n",
        "ws_mavg = w_mavg['windspeed'].copy().to_frame()\n",
        "h = wea['humidity'].copy().to_frame()\n",
        "h_savg = w_savg['humidity'].copy().to_frame()\n",
        "h_mavg = w_mavg['humidity'].copy().to_frame()"
      ],
      "metadata": {
        "id": "5q9UxrIbkKu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rHBSH9bF3Yh"
      },
      "outputs": [],
      "source": [
        "general_data_s1 =  pd.concat([ f_mavg,  f_savg, sp, s_mavg, s_savg, wea,  w_mavg, w_savg,  temp, hol ], axis = 1)\n",
        "general_data_s2 =  pd.concat([ f_mavg,  f_savg, sp, s_mavg, s_savg,                        temp, hol ], axis = 1)\n",
        "general_data_s3 =  pd.concat([ f_mavg,  f_savg, sp, s_mavg, s_savg, wea,  w_mavg, w_savg,  temp      ], axis = 1)\n",
        "general_data_s4 =  pd.concat([                                      wea,  w_mavg, w_savg,  temp, hol ], axis = 1)\n",
        "general_data_s5 =  pd.concat([                  sp,                 wea,                   temp, hol ], axis = 1)\n",
        "general_data_s6 =  pd.concat([                  sp,                                        temp, hol ], axis = 1)\n",
        "general_data_s7 =  pd.concat([                  sp,                 wea,                   temp      ], axis = 1)\n",
        "general_data_s8 =  pd.concat([                                      wea,                   temp, hol ], axis = 1)\n",
        "general_data_s9 =  pd.concat([ f_mavg,  f_savg, sp, s_mavg, s_savg, t, t_savg, t_mavg, ws, ws_savg, ws_mavg, h, h_savg, h_mavg, hd, su], axis = 1)\n",
        "general_data_s10 = pd.concat([ sp,                                  t,                 ws,                   h,                 hd, su], axis = 1)\n",
        "\n",
        "general_data_list = [general_data_s1, general_data_s2, general_data_s3, general_data_s4, general_data_s5,\n",
        "                     general_data_s6, general_data_s7, general_data_s8, general_data_s9, general_data_s10]\n",
        "\n",
        "for i in range(len(general_data_list)):\n",
        "  print('General data ', i+1, ' shape: ', general_data_list[i].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BpY4eng0A59"
      },
      "outputs": [],
      "source": [
        "flow_scaler = MinMaxScaler()\n",
        "speed_scaler = MinMaxScaler()\n",
        "general_scaler = MinMaxScaler()\n",
        "\n",
        "flow_data_scaled = pd.DataFrame(flow_scaler.fit_transform(fl).astype(float), columns=fl.columns)\n",
        "\n",
        "\n",
        "general_data_scaled_list = []\n",
        "\n",
        "# Iterate over the dataframes and apply scaling\n",
        "for data in general_data_list:\n",
        "    scaled_data = pd.DataFrame(general_scaler.fit_transform(data).astype(float), columns=data.columns)\n",
        "    general_data_scaled_list.append(scaled_data)\n",
        "\n",
        "joblib.dump(flow_scaler, 'flow_scaler.gz')\n",
        "# joblib.dump(speed_scaler, 'speed_scaler.gz')\n",
        "# joblib.dump(general_scaler, 'general_scaler.gz')\n",
        "\n",
        "full_data_list = []\n",
        "for i in range(len(general_data_scaled_list)):\n",
        "  if i == 3 or i == 7:\n",
        "    full_data = general_data_scaled_list[i].copy()\n",
        "    full_data_list.append(full_data)\n",
        "  else:\n",
        "    full_data = pd.concat([flow_data_scaled, general_data_scaled_list[i]], axis = 1)\n",
        "    full_data_list.append(full_data)\n",
        "  print('Full data ', i+1, ' shape: ', full_data.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpN2utfASCam"
      },
      "source": [
        "Data split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_hor = 12\n",
        "scenario = 1\n",
        "full_data = full_data_list[scenario-1]"
      ],
      "metadata": {
        "id": "giPbht-PFhW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihVSqX91m8mH"
      },
      "outputs": [],
      "source": [
        "days_for_training = int(334*0.7)\n",
        "days_for_testing = int(334*0.2) + 1\n",
        "days_for_validation = int(334*0.1)\n",
        "\n",
        "test_steps = days_for_testing * 24\n",
        "validation_steps = days_for_validation * 24\n",
        "training_steps = days_for_training * 24\n",
        "\n",
        "training_data = full_data[:training_steps]\n",
        "validation_data = full_data[training_steps : training_steps + validation_steps]\n",
        "test_data = full_data[training_steps + validation_steps :]\n",
        "\n",
        "# training_data = full_data[:training_steps]\n",
        "# validation_data = full_data[training_steps :]\n",
        "\n",
        "print(\"Training data shape: \", training_data.shape)\n",
        "print(\"Validation data shape: \", validation_data.shape)\n",
        "print(\"Test data shape: \", test_data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sequence creation function\n",
        "def create_nodata_sequences(inputs, labels, n_steps_in, n_steps_out):\n",
        "    X, y = [], []\n",
        "    for i in range(len(inputs) - n_steps_in - n_steps_out + 1):\n",
        "        seq_x = inputs.iloc[i:i + n_steps_in].values\n",
        "        seq_y = labels.iloc[i + n_steps_in:i + n_steps_in + n_steps_out].values\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "g8rK4cc9m6Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWZigo203n8b"
      },
      "outputs": [],
      "source": [
        "def create_sequences(data, n_steps_in, n_steps_out, num_features):\n",
        "    if n_steps_out > n_steps_in:\n",
        "        print(\"Warning: n_steps_out is greater than n_steps_in. Make sure this is intended.\")\n",
        "\n",
        "    if num_features > data.shape[1]:\n",
        "        raise ValueError(\"num_features exceeds the number of features in the data.\")\n",
        "\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - n_steps_in - n_steps_out + 1):  # Corrected loop range\n",
        "        end_ix = i + n_steps_in\n",
        "        out_end_ix = end_ix + n_steps_out\n",
        "        seq_x, seq_y = data[i:end_ix, :], data[end_ix:out_end_ix, :num_features]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvL6SWoFjYdz"
      },
      "outputs": [],
      "source": [
        "training_data_values = training_data.values\n",
        "test_data_values = test_data.values\n",
        "validation_data_values = validation_data.values\n",
        "\n",
        "n_features = full_data.shape[1]\n",
        "n_steps_in, n_steps_out = 12, pred_hor\n",
        "X_train, y_train = create_sequences(training_data_values, n_steps_in, n_steps_out, num_nodes)\n",
        "X_validation, y_validation = create_sequences(validation_data_values, n_steps_in, n_steps_out, num_nodes)\n",
        "X_test, y_test = create_sequences(test_data_values, n_steps_in, n_steps_out, num_nodes)\n",
        "\n",
        "print('X_train length: ', len(X_train), ' and shape ', X_train[1].shape)\n",
        "print('Y_train length: ', len(y_train), ' and shape ', y_train[1].shape)\n",
        "\n",
        "print('X_validation length: ', len(X_validation), ' and shape ', X_validation[1].shape)\n",
        "print('Y_validation length: ', len(y_validation), ' and shape ', y_validation[1].shape)\n",
        "\n",
        "print('X_test length: ', len(X_test), ' and shape ', X_test[1].shape)\n",
        "print('Y_test length: ', len(y_test), ' and shape ', y_test[1].shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbPrFUnh9fmM"
      },
      "source": [
        "Creating tensors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7LxIFJBkq-T"
      },
      "outputs": [],
      "source": [
        "X_train_tensor = torch.tensor(X_train).float()\n",
        "y_train_tensor = torch.tensor(y_train).float()\n",
        "X_test_tensor = torch.tensor(X_test).float()\n",
        "y_test_tensor = torch.tensor(y_test).float()\n",
        "X_validation_tensor = torch.tensor(X_validation).float()\n",
        "y_validation_tensor = torch.tensor(y_validation).float()\n",
        "\n",
        "train_data_final = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_data_final = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "validation_data_final = TensorDataset(X_validation_tensor, y_validation_tensor)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_data_final, shuffle=False, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data_final, shuffle=False, batch_size=batch_size)\n",
        "validation_loader = DataLoader(validation_data_final, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "print(X_train_tensor.shape)\n",
        "print(y_train_tensor.shape)\n",
        "print(X_test_tensor.shape)\n",
        "print(y_test_tensor.shape)\n",
        "print(X_validation_tensor.shape)\n",
        "print(y_validation_tensor.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SIT1lmbsG2L"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ATT-Bi-LSTM"
      ],
      "metadata": {
        "id": "uQaoDvbeMmR4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6752vqIrNrh"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.attention_weights = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
        "        self.context_vector = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        attention_scores = self.context_vector(torch.tanh(self.attention_weights(lstm_output)))\n",
        "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
        "        context_vector = attention_weights * lstm_output\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "        return context_vector\n",
        "\n",
        "class ATT_BiLSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, pred_horizon):\n",
        "        super(ATT_BiLSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.pred_horizon = pred_horizon\n",
        "\n",
        "        # Bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.attention = AttentionLayer(hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim * pred_horizon)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim).to(x.device)\n",
        "\n",
        "        # LSTM layer\n",
        "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Apply attention\n",
        "        context_vector = self.attention(lstm_out)\n",
        "\n",
        "        # Fully connected layer\n",
        "        out = self.fc(context_vector)\n",
        "\n",
        "        # Activation\n",
        "        predicted_flows = self.activation(out)\n",
        "\n",
        "        # Reshape to have the prediction horizon\n",
        "        predicted_flows = predicted_flows.view(batch_size, self.pred_horizon, -1)\n",
        "        return predicted_flows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yyuh0Oifd6OI"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_features = X_train_tensor.shape[2]\n",
        "output_features = y_train_tensor.shape[2]\n",
        "pred_horizon = n_steps_out\n",
        "\n",
        "# Initialize the model with the desired prediction horizon\n",
        "model = ATT_BiLSTMModel(input_dim=input_features, hidden_dim=300, num_layers=3, output_dim=output_features, pred_horizon=pred_horizon)\n",
        "model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropout model"
      ],
      "metadata": {
        "id": "BCQI7YvGRuR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.attention_weights = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
        "        self.context_vector = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        attention_scores = self.context_vector(torch.tanh(self.attention_weights(lstm_output)))\n",
        "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
        "        context_vector = attention_weights * lstm_output\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "        return context_vector\n",
        "\n",
        "class ATT_BiLSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, pred_horizon, dropout):\n",
        "        super(ATT_BiLSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.pred_horizon = pred_horizon\n",
        "\n",
        "        # Bidirectional LSTM with dropout\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        self.attention = AttentionLayer(hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim * pred_horizon)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim).to(x.device)  # 2 for bidirection\n",
        "        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim).to(x.device)\n",
        "\n",
        "        # LSTM layer\n",
        "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Apply attention\n",
        "        context_vector = self.attention(lstm_out)\n",
        "\n",
        "        # Fully connected layer\n",
        "        out = self.fc(context_vector)\n",
        "\n",
        "        # Activation\n",
        "        predicted_flows = self.activation(out)\n",
        "\n",
        "        # Reshape to have the prediction horizon\n",
        "        predicted_flows = predicted_flows.view(batch_size, self.pred_horizon, -1)\n",
        "        return predicted_flows"
      ],
      "metadata": {
        "id": "37xNC83HRwiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X_train_tensor and y_train_tensor are your training data tensors\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_features = X_train_tensor.shape[2]\n",
        "output_features = y_train_tensor.shape[2]\n",
        "pred_horizon = n_steps_out\n",
        "dropout_rate = 0.2\n",
        "# Initialize the model with the desired prediction horizon and dropout rate\n",
        "model = ATT_BiLSTMModel(input_dim=input_features, hidden_dim=300, num_layers=3, output_dim=output_features, pred_horizon=pred_horizon, dropout=dropout_rate)\n",
        "model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "28IJelltRzmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a10IreOvXLYP"
      },
      "source": [
        "### Loss functions definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ior2TryBU7XT"
      },
      "outputs": [],
      "source": [
        "def rmse(predictions, targets):\n",
        "    return torch.sqrt(((predictions - targets) ** 2).mean())\n",
        "\n",
        "def mae(predictions, targets):\n",
        "    return torch.abs(predictions - targets).mean()\n",
        "\n",
        "def mape_loss(outputs, labels):\n",
        "    # Avoid division by zero; replace zero actuals with a small number (epsilon)\n",
        "    epsilon = 1e-8\n",
        "    return torch.mean(torch.abs(labels - outputs) / (labels + epsilon)) * 100\n",
        "\n",
        "def mse_loss(output, label):\n",
        "    # Ensure the output and label tensors are the same shape\n",
        "    assert output.shape == label.shape, \"Output and label must have the same shape\"\n",
        "\n",
        "    # Compute the mean squared error\n",
        "    mse = torch.mean((output - label) ** 2)\n",
        "    return mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxwvcJVIwN8f"
      },
      "source": [
        "## Train and validate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Itn1Epge-phQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Define the training function with model checkpointing\n",
        "def train_model(model, train_loader, validation_loader, criterion, optimizer, num_epochs, patience):\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        total_train_loss = 0\n",
        "        # Start timer\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)  # Use the specified criterion (MSE in your case)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        total_train_loss /= len(train_loader)\n",
        "        # if (epoch + 1) % 20 == 0:\n",
        "        #     print(f\"Epoch {epoch+1} Train Loss: {total_train_loss:.6f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in validation_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        total_val_loss /= len(validation_loader)\n",
        "        # Only printing after every 20 epochs\n",
        "        # if (epoch + 1) % 20 == 0:\n",
        "        #     print(f\"Epoch {epoch+1} Validation Loss: {total_val_loss:.6f}\")\n",
        "\n",
        "        # Checkpointing and Early Stopping\n",
        "        if total_val_loss < best_val_loss:\n",
        "            best_val_loss = total_val_loss\n",
        "            patience_counter = 0\n",
        "            # Save the model checkpoint\n",
        "            torch.save(model.state_dict(), f'best_model.pth')\n",
        "            # print(f\"Epoch {epoch+1}: New best model saved with validation loss: {total_val_loss:.6f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                # print(f\"Early stopping at epoch {epoch+1} due to no improvement in validation loss for {patience} epochs.\")\n",
        "                break\n",
        "        # End timer\n",
        "        end_time = time.time()\n",
        "        time_to_train = end_time - start_time\n",
        "\n",
        "    return best_val_loss, time_to_train  # Return the best validation loss achieved\n",
        "\n",
        "\n",
        "# best_val_loss, time_to_train = train_model(model, train_loader, validation_loader, criterion, optimizer, num_epochs=20, patience=20)\n",
        "# print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
        "# print(f\"Total training time: {time_to_train:.2f} seconds\")\n",
        "\n",
        "\n",
        "# model.load_state_dict(torch.load('/content/best_model.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O7_OFjp_aUB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import joblib\n",
        "import pandas as pd  # Import pandas\n",
        "\n",
        "epsilon = 1e-4\n",
        "\n",
        "def evaluate_model_and_collect_data(model, test_loader, device, scaler_path, pred_horizon):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    errors = {\n",
        "        'mse': [[] for _ in range(pred_horizon)],\n",
        "        'rmse': [[] for _ in range(pred_horizon)],\n",
        "        'mae': [[] for _ in range(pred_horizon)],\n",
        "        'mape': [[] for _ in range(pred_horizon)]\n",
        "    }\n",
        "\n",
        "    flow_scaler = joblib.load(scaler_path)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            outputs_np = outputs.cpu().numpy()\n",
        "            labels_np = labels.cpu().numpy()\n",
        "\n",
        "            # Flatten the predictions and labels for inverse scaling\n",
        "            outputs_np_flat = outputs_np.reshape(-1, outputs_np.shape[-1])\n",
        "            labels_np_flat = labels_np.reshape(-1, labels_np.shape[-1])\n",
        "\n",
        "            # Denormalizes\n",
        "            outputs_denorm_flat = flow_scaler.inverse_transform(outputs_np_flat)\n",
        "            labels_denorm_flat = flow_scaler.inverse_transform(labels_np_flat)\n",
        "\n",
        "            # Reshape back to original shape\n",
        "            outputs_denorm = outputs_denorm_flat.reshape(outputs_np.shape)\n",
        "            labels_denorm = labels_denorm_flat.reshape(labels_np.shape)\n",
        "\n",
        "            predictions.extend(outputs_denorm)\n",
        "            actuals.extend(labels_denorm)\n",
        "\n",
        "            # Calculate errors\n",
        "            for t in range(pred_horizon):\n",
        "                mse = np.mean((outputs_denorm[:, t] - labels_denorm[:, t]) ** 2)\n",
        "                rmse = np.sqrt(mse)\n",
        "                mae = np.mean(np.abs(outputs_denorm[:, t] - labels_denorm[:, t]))\n",
        "                denominator = np.abs(outputs_denorm[:, t]) + np.abs(labels_denorm[:, t])\n",
        "                smape = np.mean(2 * np.abs(outputs_denorm[:, t] - labels_denorm[:, t]) / (denominator + epsilon)) * 100\n",
        "                smape = np.clip(smape, 0, 100)  # SMAPE can go up to 200%\n",
        "\n",
        "                errors['mse'][t].append(mse)\n",
        "                errors['rmse'][t].append(rmse)\n",
        "                errors['mae'][t].append(mae)\n",
        "                errors['mape'][t].append(smape)\n",
        "\n",
        "    # Create a DataFrame to store the mean and standard deviation of errors\n",
        "    error_stats = {\n",
        "        'Hour': [],\n",
        "        'Mean RMSE': [],\n",
        "        'Std RMSE': [],\n",
        "        'Mean MAE': [],\n",
        "        'Std MAE': [],\n",
        "        'Mean MAPE': [],\n",
        "        'Std MAPE': []\n",
        "    }\n",
        "\n",
        "    # mean and standard deviation of errors for each hour\n",
        "    for i in range(pred_horizon):\n",
        "        mean_rmse = np.mean(errors['rmse'][i])\n",
        "        std_rmse = np.std(errors['rmse'][i])\n",
        "        mean_mae = np.mean(errors['mae'][i])\n",
        "        std_mae = np.std(errors['mae'][i])\n",
        "        mean_mape = np.mean(errors['mape'][i])\n",
        "        std_mape = np.std(errors['mape'][i])\n",
        "\n",
        "        error_stats['Hour'].append(i + 1)\n",
        "        error_stats['Mean RMSE'].append(mean_rmse)\n",
        "        error_stats['Std RMSE'].append(std_rmse)\n",
        "        error_stats['Mean MAE'].append(mean_mae)\n",
        "        error_stats['Std MAE'].append(std_mae)\n",
        "        error_stats['Mean MAPE'].append(mean_mape)\n",
        "        error_stats['Std MAPE'].append(std_mape)\n",
        "\n",
        "    error_df = pd.DataFrame(error_stats)\n",
        "\n",
        "    return np.array(predictions), np.array(actuals), error_df\n",
        "\n",
        "# Evaluate the model\n",
        "# predictions, actuals, error_df = evaluate_model_and_collect_data(model, test_loader, device, 'flow_scaler.gz', pred_horizon=n_steps_out)\n",
        "\n",
        "# drive_path = '/content/drive/MyDrive/LSTM-ATT'\n",
        "\n",
        "# predictions_filename = f'predictions_s{scenario}_12_{pred_hor}.gz'\n",
        "# actuals_filename = f'actuals_s{scenario}_12_{pred_hor}.gz'\n",
        "\n",
        "# joblib.dump(predictions, predictions_filename)\n",
        "# joblib.dump(actuals, actuals_filename)\n",
        "\n",
        "# !cp {predictions_filename} {drive_path}\n",
        "# !cp {actuals_filename} {drive_path}\n",
        "\n",
        "# Display the error statistics DataFrame\n",
        "# error_df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "DPO_3rGRyDwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_path = '/content/drive/MyDrive/ATT_Bi_LSTM_RESULTS/predictions'\n",
        "actuals_path = '/content/drive/MyDrive/ATT_Bi_LSTM_RESULTS/actuals'\n",
        "error_path = '/content/drive/MyDrive/ATT_Bi_LSTM_RESULTS/errorMetrics'\n",
        "final_results_path = '/content/drive/MyDrive/ATT_Bi_LSTM_RESULTS/final_results'\n",
        "\n",
        "def save_to_drive(predictions, actuals, error_df, scenario, pred_hor):\n",
        "    predictions_filename = f'predictions_s{scenario}_12_{pred_hor}.gz'\n",
        "    actuals_filename = f'actuals_s{scenario}_12_{pred_hor}.gz'\n",
        "    error_df_filename = f'errorMetrics_s{scenario}_12_{pred_hor}.csv'\n",
        "\n",
        "    print('Saving predictions and actuals for scenario ', scenario, ' and prediction horizon ', pred_hor)\n",
        "    error_df.to_csv(error_df_filename, index=False, sep = ';')\n",
        "    joblib.dump(predictions, predictions_filename)\n",
        "    joblib.dump(actuals, actuals_filename)\n",
        "\n",
        "    !cp {predictions_filename} {predictions_path}\n",
        "    !cp {actuals_filename} {actuals_path}\n",
        "    !cp {error_df_filename} {error_path}\n",
        "\n",
        "    # Remove the files from the local environment\n",
        "    os.remove(predictions_filename)\n",
        "    os.remove(actuals_filename)\n",
        "    os.remove(error_df_filename)\n",
        "\n",
        "\n",
        "    print('Done')\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "28Pg-Ik4uiS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scenarios = [4,8]\n",
        "prediction_horizons = [3,6,12]\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create empty dataframe for times for training in current scenario and prediction horizon and best validation loss\n",
        "times_df = pd.DataFrame(columns=['Scenario', 'Prediction Horizon', 'Time'])\n",
        "best_losses_df = pd.DataFrame(columns=['Scenario', 'Prediction Horizon', 'Best Validation Loss'])\n",
        "\n",
        "for scenario in scenarios:\n",
        "  for prediction_horizon in prediction_horizons:\n",
        "    # Choose full_data for each scenario sX\n",
        "    inputs = full_data_list[scenario-1]\n",
        "    labels = flow_data_scaled.copy()\n",
        "\n",
        "    days_for_training = int(334*0.7)\n",
        "    days_for_testing = int(334*0.2) + 1\n",
        "    days_for_validation = int(334*0.1)\n",
        "\n",
        "    test_steps = days_for_testing * 24\n",
        "    validation_steps = days_for_validation * 24\n",
        "    training_steps = days_for_training * 24\n",
        "\n",
        "    inputs_train = inputs[:training_steps]\n",
        "    inputs_validation = inputs[training_steps : training_steps + validation_steps]\n",
        "    inputs_test = inputs[training_steps + validation_steps :]\n",
        "    labels_train = labels[:training_steps]\n",
        "    labels_validation = labels[training_steps : training_steps + validation_steps]\n",
        "    labels_test = labels[training_steps + validation_steps :]\n",
        "\n",
        "    n_steps_in, n_steps_out = 12, prediction_horizon\n",
        "    X_train, y_train            = create_nodata_sequences(inputs_train,       labels_train,       n_steps_in, n_steps_out)\n",
        "    X_validation, y_validation  = create_nodata_sequences(inputs_validation,  labels_validation,  n_steps_in, n_steps_out)\n",
        "    X_test, y_test              = create_nodata_sequences(inputs_test,        labels_test,        n_steps_in, n_steps_out)\n",
        "\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train).float()\n",
        "    y_train_tensor = torch.tensor(y_train).float()\n",
        "    X_test_tensor = torch.tensor(X_test).float()\n",
        "    y_test_tensor = torch.tensor(y_test).float()\n",
        "    X_validation_tensor = torch.tensor(X_validation).float()\n",
        "    y_validation_tensor = torch.tensor(y_validation).float()\n",
        "\n",
        "    train_data_final = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    test_data_final = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "    validation_data_final = TensorDataset(X_validation_tensor, y_validation_tensor)\n",
        "\n",
        "    batch_size = 32\n",
        "    train_loader = DataLoader(train_data_final, shuffle=False, batch_size=batch_size)\n",
        "    test_loader = DataLoader(test_data_final, shuffle=False, batch_size=batch_size)\n",
        "    validation_loader = DataLoader(validation_data_final, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    input_features, output_features = X_train_tensor.shape[2], y_train_tensor.shape[2]\n",
        "    dropout_rate = 0.2\n",
        "    model = ATT_BiLSTMModel(input_dim=input_features,\n",
        "                      hidden_dim = 300,\n",
        "                      num_layers = 3,\n",
        "                      output_dim = output_features,\n",
        "                      pred_horizon = prediction_horizon,\n",
        "                      dropout = dropout_rate)\n",
        "    model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    print('Training for scenario ', scenario, ' and prediction horizon ', prediction_horizon)\n",
        "    best_val_loss, time_to_train = train_model(model, train_loader, validation_loader, criterion, optimizer, num_epochs=100, patience=20)\n",
        "    print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
        "\n",
        "    # Save validation loss in dataframe\n",
        "    best_row = {'Scenario': scenario, 'Prediction Horizon': prediction_horizon, 'Best Validation Loss': best_val_loss}\n",
        "    best_losses_df = pd.concat([best_losses_df, pd.DataFrame([best_row])], ignore_index=True)\n",
        "    model.load_state_dict(torch.load('/content/best_model.pth'))\n",
        "\n",
        "    # Save time it took to train in the times dataframe\n",
        "    time_row = {'Scenario': scenario, 'Prediction Horizon': prediction_horizon, 'Time': time_to_train}\n",
        "    times_df = pd.concat([times_df, pd.DataFrame([time_row])], ignore_index=True)\n",
        "\n",
        "    print('Evaluating for scenario ', scenario, ' and prediction horizon ', prediction_horizon)\n",
        "    predictions, actuals, error_df = evaluate_model_and_collect_data(model, test_loader, device, 'flow_scaler.gz', pred_horizon=n_steps_out)\n",
        "\n",
        "    save_to_drive(predictions, actuals, error_df, scenario, prediction_horizon)\n",
        "\n",
        "#Save the times and losses dataframes as csvs\n",
        "times_df.to_csv('times_df_48.csv', index=False, sep = ';')\n",
        "best_losses_df.to_csv('best_losses_df_48.csv', index=False, sep = ';')\n",
        "\n",
        "!cp times_df_48.csv {final_results_path}\n",
        "!cp best_losses_df_48.csv {final_results_path}\n",
        "\n",
        "print('*********************************TESTS DONE!*********************************')"
      ],
      "metadata": {
        "id": "LGj6E2c2fjlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scenarios = [1,2,3,5,6,7,9,10]\n",
        "prediction_horizons = [3,6,12]\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create empty dataframe for times for training in current scenario and prediction horizon and best validation loss\n",
        "times_df = pd.DataFrame(columns=['Scenario', 'Prediction Horizon', 'Time'])\n",
        "best_losses_df = pd.DataFrame(columns=['Scenario', 'Prediction Horizon', 'Best Validation Loss'])\n",
        "\n",
        "for scenario in scenarios:\n",
        "  for prediction_horizon in prediction_horizons:\n",
        "    # Choose full_data for each scenario sX\n",
        "    full_data = full_data_list[scenario-1]\n",
        "\n",
        "    days_for_training = int(334*0.7)\n",
        "    days_for_testing = int(334*0.2) + 1\n",
        "    days_for_validation = int(334*0.1)\n",
        "\n",
        "    test_steps = days_for_testing * 24\n",
        "    validation_steps = days_for_validation * 24\n",
        "    training_steps = days_for_training * 24\n",
        "\n",
        "    training_data = full_data[:training_steps]\n",
        "    validation_data = full_data[training_steps : training_steps + validation_steps]\n",
        "    test_data = full_data[training_steps + validation_steps :]\n",
        "\n",
        "    training_data_values = training_data.values\n",
        "    test_data_values = test_data.values\n",
        "    validation_data_values = validation_data.values\n",
        "\n",
        "    n_features = full_data.shape[1]\n",
        "    n_steps_in, n_steps_out = 12, prediction_horizon\n",
        "    X_train, y_train = create_sequences(training_data_values, n_steps_in, n_steps_out, num_nodes)\n",
        "    X_validation, y_validation = create_sequences(validation_data_values, n_steps_in, n_steps_out, num_nodes)\n",
        "    X_test, y_test = create_sequences(test_data_values, n_steps_in, n_steps_out, num_nodes)\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train).float()\n",
        "    y_train_tensor = torch.tensor(y_train).float()\n",
        "    X_test_tensor = torch.tensor(X_test).float()\n",
        "    y_test_tensor = torch.tensor(y_test).float()\n",
        "    X_validation_tensor = torch.tensor(X_validation).float()\n",
        "    y_validation_tensor = torch.tensor(y_validation).float()\n",
        "\n",
        "    train_data_final = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    test_data_final = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "    validation_data_final = TensorDataset(X_validation_tensor, y_validation_tensor)\n",
        "\n",
        "    batch_size = 32\n",
        "    train_loader = DataLoader(train_data_final, shuffle=False, batch_size=batch_size)\n",
        "    test_loader = DataLoader(test_data_final, shuffle=False, batch_size=batch_size)\n",
        "    validation_loader = DataLoader(validation_data_final, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    input_features, output_features = X_train_tensor.shape[2], y_train_tensor.shape[2]\n",
        "    dropout_rate = 0.2\n",
        "    model = ATT_BiLSTMModel(input_dim=input_features,\n",
        "                      hidden_dim = 300,\n",
        "                      num_layers = 3,\n",
        "                      output_dim = output_features,\n",
        "                      pred_horizon = prediction_horizon,\n",
        "                      dropout = dropout_rate)\n",
        "    model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    print('Training for scenario ', scenario, ' and prediction horizon ', prediction_horizon)\n",
        "    best_val_loss, time_to_train = train_model(model, train_loader, validation_loader, criterion, optimizer, num_epochs=100, patience=20)\n",
        "    print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
        "\n",
        "    # Save validation loss in dataframe\n",
        "    best_row = {'Scenario': scenario, 'Prediction Horizon': prediction_horizon, 'Best Validation Loss': best_val_loss}\n",
        "    best_losses_df = pd.concat([best_losses_df, pd.DataFrame([best_row])], ignore_index=True)\n",
        "    model.load_state_dict(torch.load('/content/best_model.pth'))\n",
        "\n",
        "    # Save time it took to train in the times dataframe\n",
        "    time_row = {'Scenario': scenario, 'Prediction Horizon': prediction_horizon, 'Time': time_to_train}\n",
        "    times_df = pd.concat([times_df, pd.DataFrame([time_row])], ignore_index=True)\n",
        "\n",
        "    print('Evaluating for scenario ', scenario, ' and prediction horizon ', prediction_horizon)\n",
        "    predictions, actuals, error_df = evaluate_model_and_collect_data(model, test_loader, device, 'flow_scaler.gz', pred_horizon=n_steps_out)\n",
        "\n",
        "    save_to_drive(predictions, actuals, error_df, scenario, prediction_horizon)\n",
        "\n",
        "#Save the times and losses dataframes as csvs\n",
        "times_df.to_csv('times_df.csv', index=False, sep = ';')\n",
        "best_losses_df.to_csv('best_losses_df.csv', index=False, sep = ';')\n",
        "\n",
        "!cp times_df.csv {final_results_path}\n",
        "!cp best_losses_df.csv {final_results_path}\n",
        "\n",
        "print('*********************************TESTS DONE!*********************************')"
      ],
      "metadata": {
        "id": "luLL2iQwc9WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make a loop that changes the separation in all the error metrics csv files to ';'\n",
        "for scenario in scenarios:\n",
        "  for prediction_horizon in prediction_horizons:\n",
        "    df = pd.read_csv(f'errorMetrics_s{scenario}_12_{prediction_horizon}.csv')\n",
        "    df.to_csv(f'errorMetrics_s{scenario}_12_{prediction_horizon}.csv', index=False, sep=';')"
      ],
      "metadata": {
        "id": "vJaQZlgdGEsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mBLxjfqVSXk"
      },
      "source": [
        "### Gathering and preparing data for plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdAdAA07Jfwd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.dates as mdates\n",
        "import datetime\n",
        "\n",
        "#Loop to plot the actuals vs the predictions of each scenario for the same pred_horizon in the same plot\n",
        "predictions_s1 = joblib.load('predictions_s1_12_3.gz')\n",
        "predictions_s2 = joblib.load('predictions_s2_12_3.gz')\n",
        "predictions_s3 = joblib.load('predictions_s3_12_3.gz')\n",
        "actuals = joblib.load('actuals_s1_12_3.gz')\n",
        "# Plotting everythin in the same graph for a certain sensor and a certain range\n",
        "sensor_id = 10\n",
        "start_date = 25*24\n",
        "end_date = 31*24\n",
        "show_day_of_week = True  # Set to True to show day of the week instead of full date\n",
        "sensor_predictions_s1 = predictions_s1[start_date:end_date, 0, sensor_id-1]\n",
        "sensor_predictions_s2 = predictions_s2[start_date:end_date, 0, sensor_id-1]\n",
        "sensor_predictions_s3 = predictions_s3[start_date:end_date, 0, sensor_id-1]\n",
        "sensor_actuals = actuals[start_date:end_date, 0, sensor_id-1]\n",
        "dates = [datetime.datetime(2023, 10, 19) + datetime.timedelta(hours=i) for i in range(start_date, end_date)]\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(dates, sensor_predictions_s1, label='Predictions S1', marker='', linestyle='dashed')\n",
        "plt.plot(dates, sensor_predictions_s2, label='Predictions S2', marker='', linestyle='dashdot')\n",
        "plt.plot(dates, sensor_predictions_s3, label='Predictions S3', marker='', linestyle='dotted')\n",
        "plt.plot(dates, sensor_actuals, label='Actual Values', marker='', linestyle='-')\n",
        "plt.title(f\"Predictions vs Actual Values for Sensor {sensor_id}\")\n",
        "plt.xlabel('Days')\n",
        "plt.ylabel('Flow (Veh/h)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "if show_day_of_week:\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%A'))  # Day of the week\n",
        "else:\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
        "\n",
        "    # Rotation for dates\n",
        "plt.gca().xaxis.set_major_locator(mdates.DayLocator())\n",
        "plt.gcf().autofmt_xdate()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7c8v_ahJTxW"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.dates as mdates\n",
        "import datetime\n",
        "\n",
        "def plot_sensor_data(predictions, actuals, sensor_id, start_date, end_date, show_day_of_week=False):\n",
        "    sensor_predictions = predictions[start_date:end_date, 0, sensor_id-1]\n",
        "    sensor_actuals = actuals[start_date:end_date, 0, sensor_id-1]\n",
        "\n",
        "    # Generate a range of dates\n",
        "    dates = [datetime.datetime(2023, 10, 19) + datetime.timedelta(hours=i) for i in range(start_date, end_date)]\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    plt.plot(dates, sensor_predictions, label='Predictions', marker='', linestyle='-')\n",
        "    plt.plot(dates, sensor_actuals, label='Actual Values', marker='', linestyle='-')\n",
        "    plt.title(f\"Predictions vs Actual Values for Sensor {sensor_id}\")\n",
        "    plt.xlabel('Days')\n",
        "    plt.ylabel('Flow (Veh/h)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    if show_day_of_week:\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%A'))  # Day of the week\n",
        "    else:\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))  # Full date\n",
        "\n",
        "    plt.gca().xaxis.set_major_locator(mdates.DayLocator())\n",
        "    plt.gcf().autofmt_xdate()  # Rotation for dates\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "sensor_id = 5\n",
        "start_date = 25*24\n",
        "end_date = 31*24\n",
        "show_day_of_week = True  # Set to True to show day of the week instead of full date\n",
        "\n",
        "plot_sensor_data(predictions, actuals, sensor_id, start_date, end_date, show_day_of_week)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KZJzUvpFopa"
      },
      "source": [
        "## Hyperparameter optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "y5w1le-sRO8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Hyperparameters to test\n",
        "dropout_rates = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "\n",
        "\n",
        "input_features = X_train_tensor.shape[2]\n",
        "output_features = y_train_tensor.shape[2]\n",
        "pred_horizon = 12\n",
        "# Store results for plotting\n",
        "results = []\n",
        "\n",
        "# Setting the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "for dropout_rate in dropout_rates:\n",
        "    model = ATT_BiLSTMModel(input_dim=input_features, hidden_dim=300, num_layers=3, output_dim=output_features, pred_horizon=pred_horizon, dropout=dropout_rate)\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    print(f\"Training with dropout_rate={dropout_rate}\")\n",
        "    best_val_loss, time_to_train  = train_model(model, train_loader, validation_loader, criterion, optimizer, num_epochs=100, patience=20)\n",
        "    print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
        "    results.append({\n",
        "        'dropout_rate': dropout_rate,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'time_to_train': time_to_train\n",
        "    })\n",
        "\n",
        "# Convert results to DataFrame for analysis\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df"
      ],
      "metadata": {
        "id": "9d92a4APXSQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.to_csv('hyperparameters_v2.csv', index=False, sep = ';')\n",
        "!cp hyperparameters_v2.csv /content/drive/MyDrive/results"
      ],
      "metadata": {
        "id": "2GJ_BNXAYPCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CikdEKFEFLlA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Hyperparameters to test\n",
        "hidden_dims = [100, 200, 300, 400]\n",
        "num_layers = [1, 2, 3, 4]\n",
        "batch_sizes = [32, 64, 128, 256]\n",
        "learning_rates = [0.0001, 0.0005, 0.001]\n",
        "\n",
        "input_features = X_train_tensor.shape[2]\n",
        "output_features = y_train_tensor.shape[2]\n",
        "pred_horizon = 12\n",
        "# Store results for plotting\n",
        "results = []\n",
        "\n",
        "# Setting the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop\n",
        "for batch_size in batch_sizes:\n",
        "    train_loader = DataLoader(train_data_final, shuffle=False, batch_size=batch_size)\n",
        "    validation_loader = DataLoader(validation_data_final, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "    for hidden_dim in hidden_dims:\n",
        "        for num_layer in num_layers:\n",
        "            for lr in learning_rates:\n",
        "                model = LSTMModel(input_dim=input_features, hidden_dim=hidden_dim, num_layers=num_layer, output_dim=output_features, pred_horizon=pred_horizon)\n",
        "                model.to(device)\n",
        "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                criterion = nn.MSELoss()\n",
        "\n",
        "                print(f\"Training with batch_size={batch_size}, hidden_dim={hidden_dim}, num_layers={num_layer}, learning_rate={lr}\")\n",
        "                best_val_loss, time_to_train  = train_model(model, train_loader, validation_loader, criterion, optimizer, num_epochs=100, patience=20)\n",
        "\n",
        "                results.append({\n",
        "                    'batch_size': batch_size,\n",
        "                    'hidden_dim': hidden_dim,\n",
        "                    'num_layers': num_layer,\n",
        "                    'learning_rate': lr,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'time_to_train': time_to_train\n",
        "                })\n",
        "\n",
        "# Convert results to DataFrame for analysis\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.to_csv('hyperparameters_v2.csv', index=False, sep = ';')\n",
        "!cp hyperparameters_v2.csv /content/drive/MyDrive/results"
      ],
      "metadata": {
        "id": "zLTcGXBPpxXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLadTh2UX-Yg"
      },
      "source": [
        "## Device information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntVF4CIGYBbE"
      },
      "outputs": [],
      "source": [
        "# Check CPU information\n",
        "print(\"CPU Information:\")\n",
        "!lscpu\n",
        "\n",
        "# Check GPU information\n",
        "print(\"\\nGPU Information:\")\n",
        "!nvidia-smi\n",
        "\n",
        "# Check RAM information\n",
        "print(\"\\nRAM Information:\")\n",
        "!free -h\n",
        "\n",
        "# Check disk space\n",
        "print(\"\\nDisk Space Information:\")\n",
        "!df -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGopVXVY7M7x"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Check for NaNs or Infs in actuals and predictions\n",
        "print(\"NaNs in actuals:\", np.isnan(actuals).any())\n",
        "print(\"NaNs in predictions:\", np.isnan(predictions).any())\n",
        "print(\"Infs in actuals:\", np.isinf(actuals).any())\n",
        "print(\"Infs in predictions:\", np.isinf(predictions).any())\n",
        "\n",
        "# Check for minimum values in actuals and predictions\n",
        "print(\"Min value in actuals:\", np.min(actuals))\n",
        "print(\"Min value in predictions:\", np.min(predictions))\n",
        "print(\"Max value in actuals:\", np.max(actuals))\n",
        "print(\"Max value in predictions:\", np.max(predictions))\n",
        "\n",
        "# Check the shapes of actuals and predictions\n",
        "print(\"Shape of actuals:\", actuals.shape)\n",
        "print(\"Shape of predictions:\", predictions.shape)\n",
        "\n",
        "print(\"Number of zeros in actuals:\", (actuals == 0).sum())\n",
        "print(\"Number of zeros in predictions:\", (predictions == 0).sum())\n",
        "print(\"Percentage of zeros in actuals\", (actuals == 0).sum() / actuals.size * 100)\n",
        "print(\"Percentage of zeros in predictions\", (predictions == 0).sum() / predictions.size * 100)\n",
        "\n",
        "# Calculate MAPE with added epsilon to avoid division by zero\n",
        "epsilon = 1e-10\n",
        "mape = np.abs((actuals - predictions) / (actuals + epsilon)) * 100\n",
        "\n",
        "print(\"percentage of MAPE over 100\", (mape > 100).sum() / mape.size * 100)\n",
        "\n",
        "# Check for any unexpected negative values\n",
        "print(\"Minimum MAPE value:\", mape.min())\n",
        "print(\"Maximum MAPE value:\", mape.max())\n",
        "print(\"Sample MAPE values:\", mape.flatten()[:10])\n",
        "\n",
        "# Debugging the problematic MAPE calculation\n",
        "# Find the specific instances where MAPE is negative\n",
        "problematic_indices = np.where((np.abs((actuals - predictions) / (actuals + epsilon)) * 100) < 0)\n",
        "print(\"Problematic indices:\", problematic_indices)\n",
        "\n",
        "# Examine the values at problematic indices\n",
        "print(\"Actuals at problematic indices:\", actuals[problematic_indices])\n",
        "print(\"Predictions at problematic indices:\", predictions[problematic_indices])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8GvIVMOEXN9"
      },
      "outputs": [],
      "source": [
        "zeros_count_total = (training_data == 0).sum().sum()\n",
        "print(\"Total number of zeros in the DataFrame:\", zeros_count_total)\n",
        "columns_with_zeros = (training_data == 0).any().sum()\n",
        "\n",
        "print(\"Number of columns with at least one zero:\", columns_with_zeros)\n",
        "\n",
        "mask = (test_data == 0).any()\n",
        "\n",
        "columns_with_zeros = training_data.columns[mask]\n",
        "\n",
        "print(\"Columns containing all one zero:\", list(columns_with_zeros))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "4MhsNAJWMguU",
        "HExeAjguMjd8",
        "JqlV2QXiMzln"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}